{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0uj5lswSMOh"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VI-BcgadHYXD",
    "outputId": "55d98a39-13aa-47b4-eb38-e0a9ec363b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Libraries imported\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import random\n",
    "import os\n",
    "from os import environ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import cv2\n",
    "import h5py\n",
    "from skimage import color\n",
    "from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import plot\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.initializers import random_uniform\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, ConvLSTM2D,Activation, Dropout, Flatten, Conv2D, MaxPooling2D, Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalAveragePooling2D, TimeDistributed, Reshape, Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "from tensorflow.initializers import random_uniform\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "print(\"Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x68AItTwSTln"
   },
   "source": [
    "## User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xESWvbaASGyG"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        pass\n",
    "        #print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "def encode_label(target):\n",
    "    res=[]\n",
    "    if target==\"N\":\n",
    "        res=0\n",
    "    elif target==\"A\":\n",
    "        res=1\n",
    "    elif target==\"R\":\n",
    "        res=2   \n",
    "    elif target==\"O\":\n",
    "        res=3\n",
    "    mat = tf.keras.utils.to_categorical(res, num_classes=4)\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE0GjIm6SXMm"
   },
   "source": [
    "## Specify data location & Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2t8Mr5gHlQ4",
    "outputId": "4f7f8cf1-cc39-49b3-a665-c5f98b38bae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Drive mounted\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive', force_remount=True)\n",
    "train_dir = '/content/drive/My Drive/Neural Network Project/TrainingSet4/'\n",
    "batch_size=20\n",
    "num_classes=4\n",
    "epochs=50\n",
    "save_dir='/content/drive/My Drive/Neural Network Project/'\n",
    "model_name=\"FK_model.h5\"\n",
    "\n",
    "print(\"Drive mounted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJerzw-MScyg"
   },
   "source": [
    "## Import the transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "588vcJqTHonH",
    "outputId": "30657a92-3ad6-4d5e-d7ab-bf4d517b90d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "x_train=[]\n",
    "y_train=[]\n",
    "\n",
    "print(\"Loading Dataset...\")\n",
    "\n",
    "for target in os.listdir(train_dir):\n",
    "    for file in os.listdir(train_dir+'/'+target):\n",
    "        label=encode_label(target)\n",
    "        y_train.append(label)\n",
    "        image = Image.open(train_dir + '/' + target + '/' + file).convert(\"RGB\")\n",
    "        image = image.resize((300, 200), Image.ANTIALIAS)\n",
    "        x = img_to_array(image)\n",
    "        x_train.append(x)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JO7m1M7dSknu"
   },
   "source": [
    "## Reshape and Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InDnmbiZHrKW"
   },
   "outputs": [],
   "source": [
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255\n",
    "y_train_cat=np.argmax(y_train, 1)\n",
    "print(y_train_cat)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, stratify=y_train_cat)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDJaFwaCSq6B"
   },
   "source": [
    "## Define the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgcJ4pIHYazr"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "#First Convolutional Layer\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "#Second Convolutional Layer\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#Third Convolutional Layer\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "#Fourth Convolutional Layer\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Fifth Convolutional Layer\n",
    "model.add(Conv2D(96, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "#Sixth Convolutional Layer\n",
    "model.add(Conv2D(96, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Seventh Convolutional Layer\n",
    "model.add(Conv2D(128, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "#Eight Convolutional Layer\n",
    "model.add(Conv2D(128, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Flatten layer\n",
    "model.add(Flatten())\n",
    "\n",
    "#First Dense Layer\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Output Layer\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "#Model summary\n",
    "print(model.summary())\n",
    "\n",
    "#Compile the model\n",
    "opt=optimizers.SGD()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5oJFnLMTFoI"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJNvYrqLHxW3"
   },
   "outputs": [],
   "source": [
    "stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=4)\n",
    "history=model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_val, y_val), shuffle=True, callbacks=[stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgVKKY7XTJ9N"
   },
   "source": [
    "## Performance metrics plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9P1oLQCdKBDc"
   },
   "outputs": [],
   "source": [
    "preds= model.predict(x_val)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "cm = confusion_matrix(y_val, y_pred, labels=None, sample_weight=None)\n",
    "print(cm)\n",
    "y_pred = keras.utils.to_categorical(y_pred, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "print(classification_report(y_val, y_pred, labels=None, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6kSiWRiTODJ"
   },
   "source": [
    "## Confusion matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dYqj_-DdgZV"
   },
   "outputs": [],
   "source": [
    "preds= model.predict(x_val)\n",
    "y_pred = np.argmax(preds, axis=1)\n",
    "y_val = np.argmax(y_val, axis=1)\n",
    "\n",
    "\n",
    "plot_confusion_matrix(y_val, y_pred, classes = np.array(['N', 'A', '~', 'O']), normalize=False)\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "conf = [] # data structure for confusions: list of (i,j,cm[i][j])\n",
    "for i in range(0,cm.shape[0]):\n",
    "    for j in range(0,cm.shape[1]):\n",
    "        if (i!=j and cm[i][j]>0):\n",
    "            conf.append([i,j,cm[i][j]])\n",
    "\n",
    "col=2\n",
    "conf = np.array(conf)\n",
    "conf = conf[np.argsort(-conf[:,col])]  # decreasing order by 3-rd column (i.e., cm[i][j])\n",
    "\n",
    "print(classification_report(y_val, y_pred, labels=None, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUtzJhKZTViz"
   },
   "source": [
    "## Accuracy and Loss Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GV0T-jcRHzmP"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fukA2PHtH4M_"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
